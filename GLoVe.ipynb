{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras|\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip the GloVe word embeddings\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word embeddings\n",
    "words = set()\n",
    "embeddings_index = {}\n",
    "with open('glove.6B.50d.txt',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        words.add(word)\n",
    "        coeffs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cosine similarity function\n",
    "def cosine_similarity(u,v):\n",
    "    dot_product = np.dot(u,v)\n",
    "    norm_u = np.sqrt(np.sum(u**2))\n",
    "    norm_v = np.sqrt(np.sum(v**2))\n",
    "    return dot_product/(norm_u*norm_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the complete analogy function\n",
    "def complete_analogy(word_a, word_b, word_c, embeddings_index):\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    encoding_a, encoding_b, encoding_c = embeddings_index[word_a], embeddings_index[word_b], embeddings_index[word_c]\n",
    "    max_similarity = -1e5\n",
    "    best_word = None\n",
    "    for word, encoding in embeddings_index.items():\n",
    "        if word in [word_a, word_b, word_c]:\n",
    "            continue\n",
    "        similarity = cosine_similarity(encoding_b-encoding_a, encoding-encoding_c)\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            best_word = word\n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gender bias function \n",
    "gender_bias = -(embeddings_index['man']-embeddings_index['woman'] +embeddings_index['father']-embeddings_index['mother'] + embeddings_index['boy'] - embeddings_index['girl'])/3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neutralize function\n",
    "def neutralize(word,embeddings_index,gender_bias):\n",
    "    encoding = embeddings_index[word]\n",
    "    projection = np.dot(encoding,gender_bias)/np.sqrt(np.sum(gender_bias**2))\n",
    "    return encoding - projection*gender_bias/np.sqrt(np.sum(gender_bias**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the equalize function\n",
    "def equalize(pair, embeddings_index, bias_axis):\n",
    "    w1, w2 = pair\n",
    "    e_w1, e_w2 = embeddings_index[w1], embeddings_index[w2]\n",
    "    mu = (e_w1 + e_w2)/2\n",
    "    mu_B = np.dot(mu, bias_axis)/np.sqrt(np.sum(bias_axis**2))\n",
    "    mu_orth = mu - mu_B*bias_axis/np.sqrt(np.sum(bias_axis**2))\n",
    "    e_w1B = np.dot(e_w1, bias_axis)/np.sqrt(np.sum(bias_axis**2))\n",
    "    e_w2B = np.dot(e_w2, bias_axis)/np.sqrt(np.sum(bias_axis**2))\n",
    "    corrected_e_w1B = np.sqrt(np.abs(1-np.sum(mu_orth**2)))*e_w1B/np.abs(e_w1B)\n",
    "    corrected_e_w2B = np.sqrt(np.abs(1-np.sum(mu_orth**2)))*e_w2B/np.abs(e_w2B)\n",
    "    e1 = corrected_e_w1B*bias_axis/np.sqrt(np.sum(bias_axis**2)) + mu_orth\n",
    "    e2 = corrected_e_w2B*bias_axis/np.sqrt(np.sum(bias_axis**2)) + mu_orth\n",
    "    return e1, e2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ce76490c07a75ea24134e09186af2521c358252e3ea70d858a2691e4a58fda3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
